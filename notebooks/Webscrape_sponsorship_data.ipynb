{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce407187-9549-4294-af28-23ad74ad8a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Import libraries:\n",
    "\n",
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad24265-d6d8-4436-b940-86f3dda29833",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Chrome options for the WebDriver \n",
    "## To scrape US Ski Team website which has security measures in place to prevent automated access\n",
    "\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('--headless')  # Run Chrome in headless mode (without a UI)\n",
    "options.add_argument('--window-size=1920,1080')  # Set the browser window size\n",
    "options.add_argument('--no-sandbox')  # Bypass OS security model\n",
    "options.add_argument('--disable-dev-shm-usage')  # Overcome limited resource problems\n",
    "options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\")  # Set a user agent to avoid detection as a bot\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22364244-7c23-4406-8665-0c39977b8d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Repeat the following for each athlete (10 times):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684addda-8fb5-472c-8605-ae68b1c9a554",
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL to scrape sponsorship data from\n",
    "url = [\n",
    "    {'CompetitorID': 163159, 'URL': 'https://www.usskiandsnowboard.org/athletes/alex-ferreira'}\n",
    "]\n",
    "\n",
    "# Initialize list to store sponsorship data\n",
    "sponsor_data = []\n",
    "\n",
    "try:\n",
    "    # Iterate over URL\n",
    "    for athlete in athlete_urls:\n",
    "        # Print the URL being accessed for debugging purposes\n",
    "        print(f\"Accessing URL: {athlete['URL']}\")\n",
    "        \n",
    "        # Open athlete's webpage\n",
    "        driver.get(athlete['URL'])\n",
    "\n",
    "        # Wait for the sponsorship blocks to load on the page\n",
    "        WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_all_elements_located((By.CLASS_NAME, 'sponsorship-block'))\n",
    "        )\n",
    "\n",
    "        # Add small random delay, so page is fully loaded\n",
    "        time.sleep(random.uniform(2, 5))  \n",
    "\n",
    "        # Parse the loaded page source using BeautifulSoup\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "        # Find all sections containing sponsorship information\n",
    "        sponsors_sections = soup.find_all('div', class_='sponsorship-block') \n",
    "\n",
    "        # Iterate through each sponsorship block to extract sponsor details\n",
    "        for sponsors_section in sponsors_sections:\n",
    "            print(\"Sponsorship block found. Extracting sponsors...\")\n",
    "            \n",
    "            # Find all sponsor cards within the sponsorship section\n",
    "            sponsor_cards = sponsors_section.find_all('div', class_='col-xs-4')\n",
    "            \n",
    "            for card in sponsor_cards:\n",
    "                sponsor_link = card.find('a')  # Find the anchor tag within the sponsor card\n",
    "                if sponsor_link:\n",
    "                    title = sponsor_link.get('title', 'No title available')  # Get the sponsor's title, default if not available\n",
    "                    link = sponsor_link.get('href', '#')  # Get the link to the sponsor's website, default if not available\n",
    "                    # Append the competitor ID, sponsor title, and link to the sponsor_data list\n",
    "                    sponsor_data.append((athlete['CompetitorID'], title, link))  \n",
    "                    \n",
    "    # Check if any sponsors were found and print them\n",
    "    if sponsor_data:\n",
    "        for sponsor in sponsor_data:\n",
    "            print(f\"Competitor ID: {sponsor[0]}, Sponsor: {sponsor[1]}, Link: {sponsor[2]}\")\n",
    "    else:\n",
    "        print(\"No sponsors found in the sponsorship sections.\")\n",
    "\n",
    "except Exception as e:\n",
    "    # Print any errors that occur during the scraping process\n",
    "    print(f\"An error occurred: {e}\")\n",
    "finally:\n",
    "    # Ensure that the WebDriver is closed regardless of success or failure\n",
    "    driver.quit()\n",
    "\n",
    "# Create DataFrame from collected sponsor data for each individual athlete\n",
    "df = pd.DataFrame(sponsor_data, columns=['Competitor ID', 'Sponsor Title', 'Link'])\n",
    "\n",
    "# Save the dataset to a new CSV file\n",
    "df.to_csv('alexsponsor.csv', index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e88762-19da-4389-9f0b-34f4900e05ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = [{'CompetitorID': 190662, 'URL': 'https://www.usskiandsnowboard.org/athletes/birk-irving'}]\n",
    "sponsor_data = []\n",
    "\n",
    "try:\n",
    "    for athlete in athlete_urls:\n",
    "        print(f\"Accessing URL: {athlete['URL']}\")\n",
    "        driver.get(athlete['URL'])\n",
    "        WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_all_elements_located((By.CLASS_NAME, 'sponsorship-block'))\n",
    "        )\n",
    "        time.sleep(random.uniform(2, 5))  \n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        sponsors_sections = soup.find_all('div', class_='sponsorship-block') \n",
    "\n",
    "        for sponsors_section in sponsors_sections:\n",
    "            print(\"Sponsorship block found. Extracting sponsors...\")\n",
    "            sponsor_cards = sponsors_section.find_all('div', class_='col-xs-4')\n",
    "            for card in sponsor_cards:\n",
    "                sponsor_link = card.find('a') \n",
    "                if sponsor_link:\n",
    "                    title = sponsor_link.get('title', 'No title available') \n",
    "                    link = sponsor_link.get('href', '#') \n",
    "                    sponsor_data.append((athlete['CompetitorID'], title, link))   \n",
    "    if sponsor_data:\n",
    "        for sponsor in sponsor_data:\n",
    "            print(f\"Competitor ID: {sponsor[0]}, Sponsor: {sponsor[1]}, Link: {sponsor[2]}\")\n",
    "    else:\n",
    "        print(\"No sponsors found in the sponsorship sections.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "finally:\n",
    "    driver.quit()\n",
    "\n",
    "df = pd.DataFrame(sponsor_data, columns=['Competitor ID', 'Sponsor Title', 'Link'])\n",
    "df.to_csv('birksponsor.csv', index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18076aa0-ad1c-4256-a754-546448cdc801",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = [{'CompetitorID': 169927, 'URL': 'https://www.usskiandsnowboard.org/athletes/aaron-blunck'}]\n",
    "sponsor_data = []\n",
    "\n",
    "try:\n",
    "    for athlete in athlete_urls:\n",
    "        print(f\"Accessing URL: {athlete['URL']}\")\n",
    "        driver.get(athlete['URL'])\n",
    "        WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_all_elements_located((By.CLASS_NAME, 'sponsorship-block'))\n",
    "        )\n",
    "        time.sleep(random.uniform(2, 5))  \n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        sponsors_sections = soup.find_all('div', class_='sponsorship-block') \n",
    "\n",
    "        for sponsors_section in sponsors_sections:\n",
    "            print(\"Sponsorship block found. Extracting sponsors...\")\n",
    "            sponsor_cards = sponsors_section.find_all('div', class_='col-xs-4')\n",
    "            for card in sponsor_cards:\n",
    "                sponsor_link = card.find('a') \n",
    "                if sponsor_link:\n",
    "                    title = sponsor_link.get('title', 'No title available') \n",
    "                    link = sponsor_link.get('href', '#') \n",
    "                    sponsor_data.append((athlete['CompetitorID'], title, link))   \n",
    "    if sponsor_data:\n",
    "        for sponsor in sponsor_data:\n",
    "            print(f\"Competitor ID: {sponsor[0]}, Sponsor: {sponsor[1]}, Link: {sponsor[2]}\")\n",
    "    else:\n",
    "        print(\"No sponsors found in the sponsorship sections.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "finally:\n",
    "    driver.quit()\n",
    "df = pd.DataFrame(sponsor_data, columns=['Competitor ID', 'Sponsor Title', 'Link'])\n",
    "df.to_csv('aaronsponsor.csv', index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0db369d-c1f6-4121-b2d3-25c52aa90545",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = [{'CompetitorID': 234610, 'URL': 'https://www.usskiandsnowboard.org/athletes/hanna-faulhaber'}]\n",
    "sponsor_data = []\n",
    "\n",
    "try:\n",
    "    for athlete in athlete_urls:\n",
    "        print(f\"Accessing URL: {athlete['URL']}\")\n",
    "        driver.get(athlete['URL'])\n",
    "        WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_all_elements_located((By.CLASS_NAME, 'sponsorship-block'))\n",
    "        )\n",
    "        time.sleep(random.uniform(2, 5))  \n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        sponsors_sections = soup.find_all('div', class_='sponsorship-block') \n",
    "\n",
    "        for sponsors_section in sponsors_sections:\n",
    "            print(\"Sponsorship block found. Extracting sponsors...\")\n",
    "            sponsor_cards = sponsors_section.find_all('div', class_='col-xs-4')\n",
    "            for card in sponsor_cards:\n",
    "                sponsor_link = card.find('a') \n",
    "                if sponsor_link:\n",
    "                    title = sponsor_link.get('title', 'No title available') \n",
    "                    link = sponsor_link.get('href', '#') \n",
    "                    sponsor_data.append((athlete['CompetitorID'], title, link))   \n",
    "    if sponsor_data:\n",
    "        for sponsor in sponsor_data:\n",
    "            print(f\"Competitor ID: {sponsor[0]}, Sponsor: {sponsor[1]}, Link: {sponsor[2]}\")\n",
    "    else:\n",
    "        print(\"No sponsors found in the sponsorship sections.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "finally:\n",
    "    driver.quit()\n",
    "df = pd.DataFrame(sponsor_data, columns=['Competitor ID', 'Sponsor Title', 'Link'])\n",
    "df.to_csv('hannasponsor.csv', index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ccbc319-341c-4824-a3a9-d2e767ff8211",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = [{'CompetitorID': 186707, 'URL': 'https://www.usskiandsnowboard.org/athletes/hunter-hess'}]\n",
    "sponsor_data = []\n",
    "\n",
    "try:\n",
    "    for athlete in athlete_urls:\n",
    "        print(f\"Accessing URL: {athlete['URL']}\")\n",
    "        driver.get(athlete['URL'])\n",
    "        WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_all_elements_located((By.CLASS_NAME, 'sponsorship-block'))\n",
    "        )\n",
    "        time.sleep(random.uniform(2, 5))  \n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        sponsors_sections = soup.find_all('div', class_='sponsorship-block') \n",
    "\n",
    "        for sponsors_section in sponsors_sections:\n",
    "            print(\"Sponsorship block found. Extracting sponsors...\")\n",
    "            sponsor_cards = sponsors_section.find_all('div', class_='col-xs-4')\n",
    "            for card in sponsor_cards:\n",
    "                sponsor_link = card.find('a') \n",
    "                if sponsor_link:\n",
    "                    title = sponsor_link.get('title', 'No title available') \n",
    "                    link = sponsor_link.get('href', '#') \n",
    "                    sponsor_data.append((athlete['CompetitorID'], title, link))   \n",
    "    if sponsor_data:\n",
    "        for sponsor in sponsor_data:\n",
    "            print(f\"Competitor ID: {sponsor[0]}, Sponsor: {sponsor[1]}, Link: {sponsor[2]}\")\n",
    "    else:\n",
    "        print(\"No sponsors found in the sponsorship sections.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "finally:\n",
    "    driver.quit()\n",
    "df = pd.DataFrame(sponsor_data, columns=['Competitor ID', 'Sponsor Title', 'Link'])\n",
    "df.to_csv('huntersponsor.csv', index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f18fb6f-efd3-434b-ad72-3a5392d49006",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = [{'CompetitorID': 249251, 'URL': 'https://www.usskiandsnowboard.org/athletes/kate-gray'}]\n",
    "sponsor_data = []\n",
    "\n",
    "try:\n",
    "    for athlete in athlete_urls:\n",
    "        print(f\"Accessing URL: {athlete['URL']}\")\n",
    "        driver.get(athlete['URL'])\n",
    "        WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_all_elements_located((By.CLASS_NAME, 'sponsorship-block'))\n",
    "        )\n",
    "        time.sleep(random.uniform(2, 5))  \n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        sponsors_sections = soup.find_all('div', class_='sponsorship-block') \n",
    "\n",
    "        for sponsors_section in sponsors_sections:\n",
    "            print(\"Sponsorship block found. Extracting sponsors...\")\n",
    "            sponsor_cards = sponsors_section.find_all('div', class_='col-xs-4')\n",
    "            for card in sponsor_cards:\n",
    "                sponsor_link = card.find('a') \n",
    "                if sponsor_link:\n",
    "                    title = sponsor_link.get('title', 'No title available') \n",
    "                    link = sponsor_link.get('href', '#') \n",
    "                    sponsor_data.append((athlete['CompetitorID'], title, link))   \n",
    "    if sponsor_data:\n",
    "        for sponsor in sponsor_data:\n",
    "            print(f\"Competitor ID: {sponsor[0]}, Sponsor: {sponsor[1]}, Link: {sponsor[2]}\")\n",
    "    else:\n",
    "        print(\"No sponsors found in the sponsorship sections.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "finally:\n",
    "    driver.quit()\n",
    "df = pd.DataFrame(sponsor_data, columns=['Competitor ID', 'Sponsor Title', 'Link'])\n",
    "df.to_csv('katesponsor.csv', index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fbc1511-61bb-4ff8-8b4b-4abb1edb94c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = [{'CompetitorID': 236601, 'URL': 'https://www.usskiandsnowboard.org/athletes/nick-geiser'}]\n",
    "sponsor_data = []\n",
    "\n",
    "try:\n",
    "    for athlete in athlete_urls:\n",
    "        print(f\"Accessing URL: {athlete['URL']}\")\n",
    "        driver.get(athlete['URL'])\n",
    "        WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_all_elements_located((By.CLASS_NAME, 'sponsorship-block'))\n",
    "        )\n",
    "        time.sleep(random.uniform(2, 5))  \n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        sponsors_sections = soup.find_all('div', class_='sponsorship-block') \n",
    "\n",
    "        for sponsors_section in sponsors_sections:\n",
    "            print(\"Sponsorship block found. Extracting sponsors...\")\n",
    "            sponsor_cards = sponsors_section.find_all('div', class_='col-xs-4')\n",
    "            for card in sponsor_cards:\n",
    "                sponsor_link = card.find('a') \n",
    "                if sponsor_link:\n",
    "                    title = sponsor_link.get('title', 'No title available') \n",
    "                    link = sponsor_link.get('href', '#') \n",
    "                    sponsor_data.append((athlete['CompetitorID'], title, link))   \n",
    "    if sponsor_data:\n",
    "        for sponsor in sponsor_data:\n",
    "            print(f\"Competitor ID: {sponsor[0]}, Sponsor: {sponsor[1]}, Link: {sponsor[2]}\")\n",
    "    else:\n",
    "        print(\"No sponsors found in the sponsorship sections.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "finally:\n",
    "    driver.quit()\n",
    "df = pd.DataFrame(sponsor_data, columns=['Competitor ID', 'Sponsor Title', 'Link'])\n",
    "df.to_csv('nicksponsor.csv', index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed4ecd9-5802-40c4-a586-c13ed5544861",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = [{'CompetitorID': 257311, 'URL': 'https://www.usskiandsnowboard.org/athletes/piper-arnold'}]\n",
    "sponsor_data = []\n",
    "\n",
    "try:\n",
    "    for athlete in athlete_urls:\n",
    "        print(f\"Accessing URL: {athlete['URL']}\")\n",
    "        driver.get(athlete['URL'])\n",
    "        WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_all_elements_located((By.CLASS_NAME, 'sponsorship-block'))\n",
    "        )\n",
    "        time.sleep(random.uniform(2, 5))  \n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        sponsors_sections = soup.find_all('div', class_='sponsorship-block') \n",
    "\n",
    "        for sponsors_section in sponsors_sections:\n",
    "            print(\"Sponsorship block found. Extracting sponsors...\")\n",
    "            sponsor_cards = sponsors_section.find_all('div', class_='col-xs-4')\n",
    "            for card in sponsor_cards:\n",
    "                sponsor_link = card.find('a') \n",
    "                if sponsor_link:\n",
    "                    title = sponsor_link.get('title', 'No title available') \n",
    "                    link = sponsor_link.get('href', '#') \n",
    "                    sponsor_data.append((athlete['CompetitorID'], title, link))   \n",
    "    if sponsor_data:\n",
    "        for sponsor in sponsor_data:\n",
    "            print(f\"Competitor ID: {sponsor[0]}, Sponsor: {sponsor[1]}, Link: {sponsor[2]}\")\n",
    "    else:\n",
    "        print(\"No sponsors found in the sponsorship sections.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "finally:\n",
    "    driver.quit()\n",
    "df = pd.DataFrame(sponsor_data, columns=['Competitor ID', 'Sponsor Title', 'Link'])\n",
    "df.to_csv('pipersponsor.csv', index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3890b93-d02a-42e4-b1f1-904a955b88e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = [{'CompetitorID': 224238, 'URL': 'https://www.usskiandsnowboard.org/athletes/riley-jacobs'}]\n",
    "sponsor_data = []\n",
    "\n",
    "try:\n",
    "    for athlete in athlete_urls:\n",
    "        print(f\"Accessing URL: {athlete['URL']}\")\n",
    "        driver.get(athlete['URL'])\n",
    "        WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_all_elements_located((By.CLASS_NAME, 'sponsorship-block'))\n",
    "        )\n",
    "        time.sleep(random.uniform(2, 5))  \n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        sponsors_sections = soup.find_all('div', class_='sponsorship-block') \n",
    "\n",
    "        for sponsors_section in sponsors_sections:\n",
    "            print(\"Sponsorship block found. Extracting sponsors...\")\n",
    "            sponsor_cards = sponsors_section.find_all('div', class_='col-xs-4')\n",
    "            for card in sponsor_cards:\n",
    "                sponsor_link = card.find('a') \n",
    "                if sponsor_link:\n",
    "                    title = sponsor_link.get('title', 'No title available') \n",
    "                    link = sponsor_link.get('href', '#') \n",
    "                    sponsor_data.append((athlete['CompetitorID'], title, link))   \n",
    "    if sponsor_data:\n",
    "        for sponsor in sponsor_data:\n",
    "            print(f\"Competitor ID: {sponsor[0]}, Sponsor: {sponsor[1]}, Link: {sponsor[2]}\")\n",
    "    else:\n",
    "        print(\"No sponsors found in the sponsorship sections.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "finally:\n",
    "    driver.quit()\n",
    "df = pd.DataFrame(sponsor_data, columns=['Competitor ID', 'Sponsor Title', 'Link'])\n",
    "df.to_csv('rileysponsor.csv', index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a8c4ca-ddbf-4c82-a9d0-f176d2196d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = [{'CompetitorID': 213604, 'URL': 'https://www.usskiandsnowboard.org/athletes/svea-irving'}]\n",
    "sponsor_data = []\n",
    "\n",
    "try:\n",
    "    for athlete in athlete_urls:\n",
    "        print(f\"Accessing URL: {athlete['URL']}\")\n",
    "        driver.get(athlete['URL'])\n",
    "        WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_all_elements_located((By.CLASS_NAME, 'sponsorship-block'))\n",
    "        )\n",
    "        time.sleep(random.uniform(2, 5))  \n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        sponsors_sections = soup.find_all('div', class_='sponsorship-block') \n",
    "\n",
    "        for sponsors_section in sponsors_sections:\n",
    "            print(\"Sponsorship block found. Extracting sponsors...\")\n",
    "            sponsor_cards = sponsors_section.find_all('div', class_='col-xs-4')\n",
    "            for card in sponsor_cards:\n",
    "                sponsor_link = card.find('a') \n",
    "                if sponsor_link:\n",
    "                    title = sponsor_link.get('title', 'No title available') \n",
    "                    link = sponsor_link.get('href', '#') \n",
    "                    sponsor_data.append((athlete['CompetitorID'], title, link))   \n",
    "    if sponsor_data:\n",
    "        for sponsor in sponsor_data:\n",
    "            print(f\"Competitor ID: {sponsor[0]}, Sponsor: {sponsor[1]}, Link: {sponsor[2]}\")\n",
    "    else:\n",
    "        print(\"No sponsors found in the sponsorship sections.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "finally:\n",
    "    driver.quit()\n",
    "df = pd.DataFrame(sponsor_data, columns=['Competitor ID', 'Sponsor Title', 'Link'])\n",
    "df.to_csv('sveasponsor.csv', index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af5b56b-8608-4460-9933-a3488268eacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Finished webscraping every athlete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4ec196-c194-4b97-ad68-1ce9bff35351",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create one dataset for all athletes\n",
    "\n",
    "# load all csv and concat\n",
    "alex = pd.read_csv('alexsponsor.csv')\n",
    "nick = pd.read_csv('nicksponsor.csv')\n",
    "hanna = pd.read_csv('hannasponsor.csv')\n",
    "svea = pd.read_csv('sveasponsor.csv')\n",
    "kate = pd.read_csv('katesponsor.csv')\n",
    "aaron = pd.read_csv('aaronsponsor.csv')\n",
    "piper = pd.read_csv('pipersponsor.csv')\n",
    "riley = pd.read_csv('rileysponsor.csv')\n",
    "hunter = pd.read_csv('huntersponsor.csv')\n",
    "birk = pd.read_csv('birksponsor.csv')\n",
    "data = pd.concat([alex, nick, hanna, svea, kate, aaron, piper, riley, hunter, birk], ignore_index=True)\n",
    "\n",
    "# Fill missing values in 'Sponsor Title' with 'No Sponsor'\n",
    "data['Sponsor Title'] = data['Sponsor Title'].fillna('No Sponsor')\n",
    "\n",
    "# Fill missing values in 'Link' with 'No Link'\n",
    "data['Link'] = data['Link'].fillna('No Link')\n",
    "\n",
    "# Group the DataFrame by 'Competitor ID' and aggregate sponsor info\n",
    "grouped_df = data.groupby('Competitor ID').agg({\n",
    "    'Sponsor Title': lambda x: ', '.join(x.unique()),  \n",
    "    'Link': lambda x: ', '.join(x.unique())            \n",
    "}).reset_index()  \n",
    "\n",
    "# Drop the 'Link' column from the grouped DataFrame (not needed for analysis)\n",
    "grouped_df = grouped_df.drop(columns=['Link'])\n",
    "\n",
    "# Save the dataset to a new CSV file\n",
    "grouped_df.to_csv('sponsorship.csv', index=False)\n",
    "\n",
    "print(\"Dataset created with sponsorship data.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
